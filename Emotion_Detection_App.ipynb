{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU9-xuUEx9Xx"
      },
      "source": [
        "# Emotion Detection Using Hugging Face and Gradio\n",
        "\n",
        "- This Jupyter notebook contains the code for the \"Emotion Detection Using Hugging Face and Gradio\" project.\n",
        "- The objective of this project is to:\n",
        "  - Develop an application that detects and classifies emotions in text inputs using a pre-trained model on the Emotion dataset, showcasing the capabilities of Hugging Face models and Gradio interfaces.\n",
        "- The project involves several steps and each step corresponds to a separate section in this Jupyter Notebook:\n",
        "\n",
        "1. Dataset Selection and Exploration\n",
        "2. Model Selection\n",
        "3. Model Fine-tuning\n",
        "4. Model Evaluation\n",
        "5. Upload Model to Hugging Face\n",
        "6. Gradio Interface Development\n",
        "7. Deployment on Hugging Face Spaces\n",
        "\n",
        "- **Refer the [README](/readme.md) for a detailed project overview.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RzYMcD5hgd6"
      },
      "source": [
        "# Environment Setup\n",
        "\n",
        "- The code in this repository can be executed in 3 ways:\n",
        "  - on google colab\n",
        "  - using Python virtual environment\n",
        "  - using VSCode DevContainers\n",
        "- See below detailed steps for each option. But before that I will cover the packages that will be installed as part of the environment setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdS2GRkvhE8W"
      },
      "source": [
        "### Google Colab\n",
        "\n",
        "- To run this code using Google Colab, click [this link](https://colab.research.google.com/github/kanad13/Emotion_Detection_App/blob/master/Emotion_Detection_App.ipynb).\n",
        "- Once you have opened this notebook in colab, uncomment the code below and execute it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmTtsHShyXKN",
        "outputId": "245d8320-e503-4a56-9d0b-53719a41ef20"
      },
      "outputs": [],
      "source": [
        "# Install pandas and other packages\n",
        "!pip3 install -r requirements.txt\n",
        "\n",
        "# Install PyTorch and related packages separately (on colab)\n",
        "%pip install torch torchvision torchaudio\n",
        "\n",
        "# Comment above line when running on Apple Silicon. It is for Google Colab.\n",
        "# Uncomment below line when running on Apple Silicon - https://developer.apple.com/metal/pytorch/\n",
        "# !pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
        "\n",
        "# Verify the installation by importing the libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchaudio\n",
        "import accelerate\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"Torchaudio version:\", torchaudio.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw3J1TXpx9X1"
      },
      "source": [
        "### Python virtual environment\n",
        "\n",
        "- All packages needed for running this code can be deployed inside a virtual environment\n",
        "- These are the steps\n",
        "  - First clone this repo locally\n",
        "  - Then open the folder\n",
        "    - `cd emotion_detection`\n",
        "- Then create a Python virtual environment\n",
        "  - `python -m venv emotion_detection_venv`\n",
        "- Then activate the virtual environment\n",
        "  - On windows - `.\\emotion_detection_venv\\Scripts\\activate`\n",
        "  - On mac - `source emotion_detection_venv/bin/activate`\n",
        "  - And now install all dependencies\n",
        "    - `pip install -r requirements.txt`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9lttW2Zx9X1"
      },
      "source": [
        "### VSCode DevContainers\n",
        "\n",
        "- If you use [vscode devcontainers](https://code.visualstudio.com/docs/devcontainers/containers) like me, then you dont have to do anything noted above in the python virtual env section\n",
        "- I have setup the repo nicely to be launch-ready the moment you download it\n",
        "- Launch vscode, and then open the command palette (ctrl+shift+p), and then select \"Remote-Containers: Open Folder in Container\"\n",
        "- Navigate to the cloned repository folder and bam...you are done...all requirements will be automatically installed\n",
        "- You can also make modifications to these 2 files as needed\n",
        "  - [devcontainer.json](.devcontainer/devcontainer.json)\n",
        "  - [postStart.sh](.devcontainer/postStart.sh)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHjDWfLVhgd9"
      },
      "source": [
        "## Optional - Run Gradio locally\n",
        "\n",
        "- I have developed the UI for the app using Gradio. To use it locally, execute the following command:\n",
        "\n",
        "```bash\n",
        "python app.py\n",
        "```\n",
        "\n",
        "- The app will be accessible at `http://localhost:7860`.\n",
        "- When using Google Colab, the gradio will be rendered directly inside the Notebook; so you dont need to open it separately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zcEQrOzhgd-"
      },
      "source": [
        "## Packages Installed\n",
        "\n",
        "- **File location**\n",
        "  - I have put all these packages in [requirements.txt](.devcontainer/requirements.txt)\n",
        "- **Pandas**\n",
        "  - It is the most popular data manipulation and analysis library for Python.\n",
        "  - I started my Python journey with Pandas. ;) And I believe that is the case for many others.\n",
        "  - It provides data structures like DataFrame and Series, and functions for reading, writing, and transforming data.\n",
        "- **Matplotlib**\n",
        "  - I have used this library in my code for generating plots.\n",
        "  - It can also be used for plotting histograms, bar charts, and other types of graphs to visualize data.\n",
        "- **IProgress**\n",
        "  - Ideally this library need not be explicitly installed. But I keep on getting errors when it is not installed. So I have included it.\n",
        "  - It provides interactive progress bars for Jupyter Notebooks.\n",
        "- **Datasets**\n",
        "  - It is a library by Hugging Face to access datasets.\n",
        "  - It allows for easy dataset loading, preprocessing, and sharing.\n",
        "- **Accelerate**\n",
        "  - Yet another library by Hugging Face\n",
        "  - It simplifies the process of training and deploying machine learning models across different hardware configurations.\n",
        "- **Transformers**\n",
        "  - This is another library by Hugging Face.\n",
        "  - It provides pre-trained models for NLP. I have used one of them in this code viz. DistilBERT\n",
        "- **Hugging Face Hub**\n",
        "  - `huggingface_hub` allows interaction with the Hugging Face Hub.\n",
        "  - Hugging Face Hub is a platform for sharing and collaborating on machine learning models and datasets.\n",
        "  - `huggingface_hub` helps with tasks like uploading, downloading, and managing models and datasets.\n",
        "- **Gradio**\n",
        "  - It is an open-source Python library.\n",
        "  - It allows you to quickly create customizable user interfaces for machine learning models.\n",
        "  - I have created the UI for my Emotion Detection App using gradio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm4tB6zBhgd-"
      },
      "source": [
        "# Step 1 - Dataset Selection and Exploration\n",
        "\n",
        "- The \"Emotion Detection App\" involves several steps and each step corresponds to a separate section in this Jupyter Notebook:\n",
        "\n",
        "1. Dataset Selection and Exploration - `covered in this section`\n",
        "2. Model Selection\n",
        "3. Model Fine-tuning\n",
        "4. Model Evaluation\n",
        "5. Upload Model to Hugging Face\n",
        "6. Gradio Interface Development\n",
        "7. Deployment on Hugging Face Spaces\n",
        "\n",
        "- In this section, I cover the first step of the \"Emotion Detection in Text Using Hugging Face and Gradio\" project.\n",
        "- The steps covered are\n",
        "  - Use the Emotion dataset available on Hugging Face.\n",
        "  - Explore the dataset to understand its structure, including the different classes of emotions (e.g., joy, anger, sadness, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRtOTgMnhgd-"
      },
      "source": [
        "## About the Emotion Dataset\n",
        "\n",
        "- Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise.\n",
        "- For more detailed information please refer to the [dataset](https://huggingface.co/datasets/dair-ai/emotion)\n",
        "- The data fields are:\n",
        "  - `text`: a string feature\n",
        "  - `label`: a classification label, with possible values including sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5)\n",
        "- An example looks as follows.\n",
        "\n",
        "```json\n",
        "{\n",
        "\t\"text\": \"im feeling quite sad and sorry for myself but ill snap out of it soon\",\n",
        "\t\"label\": 0\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcm8R9Mgx9X2"
      },
      "source": [
        "## Importing Emotion Dataset\n",
        "\n",
        "- In this section I will import the dataset and explore its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8ElvDlrx9X3",
        "outputId": "119e798b-63ca-4ca7-9e35-0be0a2faa418"
      },
      "outputs": [],
      "source": [
        "# Import dataset\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings about logging to Hugging Face, since I plan to only use public datasets.\n",
        "warnings.filterwarnings(\"ignore\", message=\"The secret `HF_TOKEN` does not exist\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"The repository for emotion contains custom code\")\n",
        "\n",
        "# Load the Emotion dataset with trust_remote_code parameter\n",
        "emotion_dataset = load_dataset(\"emotion\", trust_remote_code=True)\n",
        "\n",
        "# Display the dataset structure\n",
        "print(emotion_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MBl8p0sx9X4"
      },
      "source": [
        "## Exploring the Dataset\n",
        "\n",
        "- To understand the dataset, I will now explore the different splits (train, validation, test) and the class distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIKIarB6x9X4",
        "outputId": "ca9c4d8b-eee0-4b7c-f8aa-a35ef58e1c20"
      },
      "outputs": [],
      "source": [
        "# Convert the dataset to Pandas DataFrame for easier exploration\n",
        "train_df = emotion_dataset['train'].to_pandas()\n",
        "validation_df = emotion_dataset['validation'].to_pandas()\n",
        "test_df = emotion_dataset['test'].to_pandas()\n",
        "\n",
        "# Display the first few rows of the training dataset\n",
        "print(train_df.head())\n",
        "\n",
        "# Display basic statistics\n",
        "print(train_df.describe())\n",
        "\n",
        "# Check the distribution of emotions in the training set\n",
        "print(train_df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJLwJ-nHx9X5"
      },
      "source": [
        "## Visualizing the Class Distribution\n",
        "\n",
        "- To visualize the class distribution, we'll plot the counts of each emotion class in the training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "3kr91ycbx9X5",
        "outputId": "eb9f363b-95f6-40b5-dd5c-192871aeae16"
      },
      "outputs": [],
      "source": [
        "# Define emotion labels\n",
        "emotion_labels = emotion_dataset['train'].features['label'].names\n",
        "\n",
        "# Plot the distribution of emotions in the training set\n",
        "train_df['label'].value_counts().plot(kind='bar', color='skyblue')\n",
        "plt.xlabel('Emotion')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Emotions in the Training Set')\n",
        "plt.xticks(ticks=range(len(emotion_labels)), labels=emotion_labels, rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stf7tc1cx9X5"
      },
      "source": [
        "## Sample Data Inspection\n",
        "\n",
        "- To get a better understanding, let me inspect some sample data points for each emotion class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t90IPPcrx9X5",
        "outputId": "b18ed775-a82a-4100-9149-c35408767cee"
      },
      "outputs": [],
      "source": [
        "# Function to print sample texts for each emotion\n",
        "def print_samples(df, num_samples=2):\n",
        "    for label in df['label'].unique():\n",
        "        samples = df[df['label'] == label].sample(num_samples)\n",
        "        emotion = emotion_labels[label]\n",
        "        print(f\"Emotion: {emotion}\")\n",
        "        for text in samples['text']:\n",
        "            print(f\" - {text}\")\n",
        "\n",
        "# Print sample texts\n",
        "print_samples(train_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqctSY8Cx9X6"
      },
      "source": [
        "# Step 2 - Model Selection\n",
        "\n",
        "- The \"Emotion Detection App\" involves several steps and each step corresponds to a separate section in this Jupyter Notebook:\n",
        "\n",
        "1. Dataset Selection and Exploration - `covered in previous section`\n",
        "2. Model Selection - `covered in this section`\n",
        "3. Model Fine-tuning\n",
        "4. Model Evaluation\n",
        "5. Upload Model to Hugging Face\n",
        "6. Gradio Interface Development\n",
        "7. Deployment on Hugging Face Spaces\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p5N5jnrx9X6"
      },
      "source": [
        "## Pre-trained Transformer Model\n",
        "\n",
        "- In this section, I will use the transformers library to select and load a pre-trained model.\n",
        "- For this task, I'll use DistilBERT, a smaller and faster version of BERT that is suitable for text classification tasks.\n",
        "- See in the next sub-section, the details about:\n",
        "  - What are Transformer Models?\n",
        "  - Pre-trained vs. Non-Pre-trained Transformer Models\n",
        "  - Major Pre-trained Transformer Models\n",
        "  - Why Did We Select DistilBERT?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ove01ZKfx9X7"
      },
      "source": [
        "## What are Transformer Models?\n",
        "\n",
        "- Transformer models are a type of neural network architecture designed for handling sequential data, such as text.\n",
        "- The key innovation of transformers is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when making predictions, enabling the handling of long-range dependencies more effectively.\n",
        "- Consider the sentence: \"The cat sat on the mat because it was soft.\" A transformer model can understand that \"it\" refers to \"the mat\" because of the self-attention mechanism, which helps the model focus on relevant parts of the sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcvF6pskx9X7"
      },
      "source": [
        "## Pre-trained vs. Non-Pre-trained Transformer Models\n",
        "\n",
        "| Feature               | Pre-trained Transformer Models                                                  | Non-Pre-trained Transformer Models                                            |\n",
        "| --------------------- | ------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |\n",
        "| **Training Time**     | Requires less time as models are pre-trained on large datasets.                 | Requires extensive training time and computational resources.                 |\n",
        "| **Performance**       | Generally achieves higher performance on specific tasks due to prior knowledge. | Performance depends heavily on the quality and quantity of the training data. |\n",
        "| **Data Requirements** | Can work well with smaller labeled datasets.                                    | Requires large amounts of labeled data for training from scratch.             |\n",
        "| **Use Case**          | Ideal for tasks where similar models have been successfully applied.            | Suitable for novel tasks or when custom architecture is required.             |\n",
        "| **Flexibility**       | Limited to the architecture of the pre-trained model.                           | Full control over model architecture and training process.                    |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgCqQCc0x9X7"
      },
      "source": [
        "### Major Pre-trained Transformer Models\n",
        "\n",
        "| Model          | Description                                                                                                                   |\n",
        "| -------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **BERT**       | Bidirectional Encoder Representations from Transformers. Designed for understanding the context of a word in search queries.  |\n",
        "| **RoBERTa**    | Robustly optimized BERT approach. Improves on BERT by training with larger mini-batches and longer sequences.                 |\n",
        "| **DistilBERT** | A smaller, faster, and cheaper version of BERT. Retains 97% of BERT's language understanding capabilities.                    |\n",
        "| **XLNet**      | Combines the best of BERT and autoregressive models like GPT, leading to state-of-the-art performance on many benchmarks.     |\n",
        "| **T5**         | Text-To-Text Transfer Transformer. Frames all NLP tasks as text-to-text problems, enabling a unified approach.                |\n",
        "| **BART**       | Bidirectional and Auto-Regressive Transformers. Combines bidirectional and autoregressive training for text generation tasks. |\n",
        "| **GPT-X**      | Generative Pre-trained Transformer 3, 3.5, 4 or 4-o. Designed for generating human-like text based on input prompts.          |\n",
        "| **Gemini**     | A state-of-the-art model by Google, known for its strong performance in various NLP tasks and advanced capabilities.          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDxNw0BQx9X7"
      },
      "source": [
        "## Why Did I Select DistilBERT?\n",
        "\n",
        "I selected DistilBERT for the following reasons:\n",
        "\n",
        "1. **Efficiency**: DistilBERT is designed to be smaller, faster, and lighter than BERT, making it more suitable for quick iterations and deployment on less powerful hardware.\n",
        "2. **Performance**: Despite its smaller size, DistilBERT retains 97% of BERT's language understanding capabilities, ensuring high performance in emotion detection tasks.\n",
        "3. **Pre-training**: DistilBERT is pre-trained on large text corpora, allowing it to leverage a rich understanding of language, which is beneficial for tasks like emotion detection with limited labeled data.\n",
        "4. **Community Support**: Being a part of the Hugging Face ecosystem, DistilBERT has extensive community support and pre-built functions for easy implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gqUx3lWx9X9"
      },
      "source": [
        "# Step 3 - Model Fine-tuning\n",
        "\n",
        "- The \"Emotion Detection App\" involves several steps and each step corresponds to a separate section in this Jupyter Notebook:\n",
        "\n",
        "1. Dataset Selection and Exploration\n",
        "2. Model Selection - `covered in previous section`\n",
        "3. Model Fine-tuning - `covered in this section`\n",
        "4. Model Evaluation\n",
        "5. Upload Model to Hugging Face\n",
        "6. Gradio Interface Development\n",
        "7. Deployment on Hugging Face Spaces\n",
        "\n",
        "- **Objective of this section**\n",
        "  - Select a pre-trained transformer model suitable for text classification tasks (in our case DistilBERT)\n",
        "  - Fine-tune the selected model on the Emotion dataset to enhance its performance for emotion detection.\n",
        "- **Detailed Steps**\n",
        "  - _Import Required Libraries_\n",
        "    - Install and import the necessary libraries for handling the dataset, model selection, and fine-tuning.\n",
        "  - _Load the Emotion Dataset_\n",
        "    - Use the Hugging Face datasets library to load the Emotion dataset.\n",
        "  - _Choose a Pre-trained Model_\n",
        "    - Select a suitable transformer model from the Hugging Face model hub, such as BERT, RoBERTa, or DistilBERT.\n",
        "  - _Preprocess the Data_\n",
        "    - Tokenize the text data.\n",
        "    - Convert the tokenized inputs into a format suitable for the model.\n",
        "  - _Loading the Pre-trained Model_\n",
        "    - Define the training parameters and fine-tune the model on the Emotion dataset.\n",
        "  - _Defining Training Arguments_\n",
        "    - Use `training_args`to specify parameters like output directory, number of epochs, batch size, warmup steps, weight decay, logging directory, and logging steps.\n",
        "  - _Defining the Trainer_\n",
        "    - Trainer is a high-level class from Hugging Face that simplifies the training and evaluation process.\n",
        "  - _Fine-tuning the Model_\n",
        "    - Use `trainer.train()`that starts the training process.\n",
        "    - It automatically handles the training loop, including the forward pass, backward pass, and optimization steps.\n",
        "  - _Saving the Model and Tokenizer_\n",
        "    - Save the fine-tuned model & tokenizer to a local directory.\n",
        "    - This ensures that the model and tokenizer can be loaded later for inference or further training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5DionGhty0s"
      },
      "source": [
        "## Choice of input parameters\n",
        "\n",
        "- I have fine-tuned this pre-trained model on colab and also on my Apple Silicon Mac.\n",
        "- Since both of them have constrained resources, I had to ensure that the fine-tuning does not consume too many resources and does not end up being executed for hours.\n",
        "- So, this section explains the choices of training parameter values I made and the reason for them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfr7BokowJB9"
      },
      "source": [
        "### Size of input dataset\n",
        "\n",
        "- The Emotions Dataset originally contains 16k labelled examples.\n",
        "- My Mac can not handle it. Neither can my colab (yes, I am too poor for colab premium)\n",
        "- So I am using smaller subset of the dataset for faster training, only 5000.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnB7OO7fuxRD"
      },
      "source": [
        "### Training Arguments\n",
        "\n",
        "- See below the training argument values I have set.\n",
        "- I have also described the ideal values for some of them.\n",
        "\n",
        "```python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # number of training epochs (ideal value 3)\n",
        "    per_device_train_batch_size=8,   # batch size for training (ideal value 16)\n",
        "    per_device_eval_batch_size=8,    # batch size for evaluation (ideal value 16)\n",
        "    warmup_steps=300,                # number of warmup steps for learning rate scheduler (ideal value 500)\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhljNs4Huirk"
      },
      "source": [
        "### Training Steps\n",
        "\n",
        "- I have not directly specified the training steps in the training arguments as mentioned above.\n",
        "- Instead, it is derived from a combination of factors, including the number of epochs, the size of the dataset, and the batch size\n",
        "- Here is how it is calculated:\n",
        "\n",
        "$$\n",
        "\\text{Total Training Steps} = \\frac{\\text{Number of Training Examples}}{\\text{Batch Size}} \\times \\text{Number of Epochs}\n",
        "$$\n",
        "\n",
        "- Given the values of my Training Arguments as mentioned above\n",
        "\n",
        "$$\n",
        "\\text{Total Training Steps} = \\frac{1000}{8} \\times 1 = 125\n",
        "$$\n",
        "\n",
        "- To summarize, the 125 training steps are a result of the number of examples from the training dataset (1000), the batch size (8), and the number of epochs (1).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245,
          "referenced_widgets": [
            "76e99c8c940e473f9e2481cd7a6af196",
            "daffd17cc11548e680aeb15fe2342fe6",
            "ec9e683e0ac84ad98b22b977a7b5c99f",
            "a00fdfa5e4ca496aa434cab686eb6db1",
            "c9dcce5da95e4ea0ac3ff4a04ef91ca4",
            "f3e5334ac6334769bec43e77884c2835",
            "de7e220a468a40fdadb6c2c66748ae07",
            "2e4d6cff7c6649178354476c86fa2edf",
            "6600f3b00ecb414492a6685a281aa8b8",
            "86f27d0a15c74bc0b2b408ba7f1685cb",
            "7acb2ba9265746d0beab32e99ef272f3"
          ]
        },
        "id": "Dy_eCaQKx9X9",
        "outputId": "ad25b693-c915-4922-b1ba-f7d0bec4ba04"
      },
      "outputs": [],
      "source": [
        "# Step 3 - Model Fine-tuning\n",
        "\n",
        "# Import required libraries\n",
        "from datasets import load_dataset  # to load datasets from Hugging Face Hub\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments  # to use pre-trained models and training utilities\n",
        "import torch  # PyTorch for tensor operations\n",
        "import numpy as np  # for numerical operations\n",
        "\n",
        "# Load the Emotion dataset from the Hugging Face Hub\n",
        "emotion_dataset = load_dataset(\"emotion\", trust_remote_code=True)\n",
        "\n",
        "# Specify the pre-trained model name to be used (DistilBERT)\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "# Load the tokenizer for the specified pre-trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)# use force_download=True when needed\n",
        "\n",
        "# Define a function to preprocess the data\n",
        "def preprocess_data(examples):\n",
        "    # Tokenize the input text, truncate to max length, and pad to max length\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
        "\n",
        "# Take a smaller subset of the training dataset for faster training\n",
        "train_dataset = emotion_dataset['train'].shuffle(seed=42).select(range(10))\n",
        "# Take a smaller subset of the validation dataset for faster evaluation\n",
        "eval_dataset = emotion_dataset['validation'].shuffle(seed=42).select(range(10))\n",
        "\n",
        "# Apply the preprocessing function to the training dataset\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
        "# Apply the preprocessing function to the validation dataset\n",
        "tokenized_eval_dataset = eval_dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "# Prepare the dataset for PyTorch by removing the 'text' column\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['text'])\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.remove_columns(['text'])\n",
        "# Rename the 'label' column to 'labels' to match the model's expectation\n",
        "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.rename_column(\"label\", \"labels\")\n",
        "# Set the format of the dataset to PyTorch tensors\n",
        "tokenized_train_dataset.set_format('torch')\n",
        "tokenized_eval_dataset.set_format('torch')\n",
        "\n",
        "# Load the pre-trained model for sequence classification with 6 output labels (emotions)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Directory to save the model and logs\n",
        "    num_train_epochs=1,              # Number of training epochs (ideal value 3)\n",
        "    per_device_train_batch_size=8,   # Batch size for training (ideal value 16)\n",
        "    per_device_eval_batch_size=8,    # Batch size for evaluation (ideal value 16)\n",
        "    warmup_steps=50,                # Number of warmup steps for learning rate scheduler (ideal value 500)\n",
        "    weight_decay=0.01,               # Strength of weight decay for regularization\n",
        "    logging_dir='./logs',            # Directory to store logs\n",
        "    logging_steps=10,                # Log every 10 steps\n",
        ")\n",
        "\n",
        "# Instantiate the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # The pre-trained model to be fine-tuned\n",
        "    args=training_args,                  # The training arguments\n",
        "    train_dataset=tokenized_train_dataset,  # The preprocessed training dataset\n",
        "    eval_dataset=tokenized_eval_dataset     # The preprocessed evaluation dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer to disk\n",
        "model.save_pretrained(\"emotion-detection-model\")\n",
        "tokenizer.save_pretrained(\"emotion-detection-tokenizer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3FGrTijDoy2"
      },
      "source": [
        "# Step 4 - Model Evaluation\n",
        "\n",
        "- The \"Emotion Detection App\" involves several steps and each step corresponds to a separate section in this Jupyter Notebook:\n",
        "\n",
        "1. Dataset Selection and Exploration\n",
        "2. Model Selection\n",
        "3. Model Fine-tuning - `covered in previous section`\n",
        "4. Model Evaluation - `covered in this section`\n",
        "5. Upload Model to Hugging Face\n",
        "6. Gradio Interface Development\n",
        "7. Deployment on Hugging Face Spaces\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9ST0gm2ZnH1"
      },
      "source": [
        "## Recap steps 1 to 3\n",
        "\n",
        "- Let me provide some detailed comments on what has each of the 3 steps achieved earlier:\n",
        "\n",
        "1. **Dataset Selection and Exploration**\n",
        "\n",
        "- I am working with the Emotion dataset from Hugging Face, which contains text inputs labeled with different emotions.\n",
        "  We explored the dataset to understand its structure and the distribution of different emotion classes.\n",
        "\n",
        "2. **Model Selection**\n",
        "\n",
        "- Then I selected a pre-trained model suitable for sequence classification tasks from Hugging Face's model hub.\n",
        "\n",
        "3. **Model Fine-tuning**\n",
        "\n",
        "- Then I fine-tuned the pre-trained model on our Emotion dataset to adapt it specifically for emotion detection in text.\n",
        "\n",
        "The code in the next cell aims to evaluate the fine-tuned model on a test set to measure its performance.\n",
        "\n",
        "I will load the saved model and tokenizer, preprocess the test data, and then use the model to make predictions.\n",
        "\n",
        "Finally, I will calculate and display the accuracy of the model on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "15W6laXnDq2J",
        "outputId": "da75f734-6a6d-41e7-dac4-39b27d374555"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute metrics\n",
        "def compute_metrics(p):\n",
        "    # Unpack predictions and true labels\n",
        "    predictions, labels = p\n",
        "    # Get the predicted class by finding the index with the highest probability\n",
        "    # The `predictions` object holds the raw predictions and true labels. We use `np.argmax` to get the predicted class with the highest probability.\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    # Calculate accuracy\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    # Generate a detailed classification report\n",
        "    report = classification_report(labels, predictions, target_names=emotion_labels)\n",
        "    return {\"accuracy\": acc, \"report\": report}\n",
        "\n",
        "\n",
        "# Evaluate the model on the evaluation dataset\n",
        "eval_result = trainer.evaluate()\n",
        "print(f\"Evaluation result: {eval_result}\")\n",
        "\n",
        "# Predict on the evaluation dataset\n",
        "predictions = trainer.predict(tokenized_eval_dataset)\n",
        "\n",
        "# Get the predicted class for each sample\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Generate and print the classification report\n",
        "report = classification_report(predictions.label_ids, preds, target_names=emotion_labels)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJyPe2iOPptk"
      },
      "source": [
        "## Interpret evaluation metrics\n",
        "\n",
        "- The code cell above prints evaluation metrics and the classification report.\n",
        "- Here are some pointers towards understanding this report:\n",
        "\n",
        "1. **Evaluation Loss**:\n",
        "\n",
        "   - `eval_loss`\n",
        "   - It contains the evaluation loss and other metrics, which we print to get a sense of model performance.\n",
        "\n",
        "2. **Evaluation Runtime**:\n",
        "\n",
        "   - `eval_runtime`\n",
        "   - This is the time taken to evaluate the model.\n",
        "\n",
        "3. **Evaluation Speed**:\n",
        "\n",
        "   - `eval_samples_per_second`\n",
        "   - `eval_steps_per_second`\n",
        "   - These metrics indicate the speed of evaluation.\n",
        "\n",
        "4. **Training Parameters**:\n",
        "\n",
        "   - `epoch`\n",
        "   - I had specified this value in Step 3.\n",
        "\n",
        "5. **Classification Report**:\n",
        "   - The classification report provides a more detailed view of the model's performance on a per-class basis.\n",
        "   - `precision`, `recall`, and `f1-score` are metrics that indicate how well the model is performing for each class (sadness, joy, etc.).\n",
        "   - Higher values indicate better quality of the fine-tuned model.\n",
        "   - The table also lists overall accuracy. It indicates e.g. the model is correctly predicting 98% of the samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJXMy3WeZtNm"
      },
      "source": [
        "# Step 5 - Upload Model to Hugging Face\n",
        "\n",
        "- The \"Emotion Detection App\" involves several steps and each step corresponds to a separate section in this Jupyter Notebook:\n",
        "\n",
        "1. Dataset Selection and Exploration\n",
        "2. Model Selection\n",
        "3. Model Fine-tuning\n",
        "4. Model Evaluation - `covered in previous section`\n",
        "5. Upload Model to Hugging Face - `covered in this section`\n",
        "6. Gradio Interface Development\n",
        "7. Deployment on Hugging Face Spaces\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asf-BO12X5ok"
      },
      "source": [
        "## Hugging Face Token\n",
        "\n",
        "- I will now log in to my Hugging Face account and secure a token as described [here](https://huggingface.co/docs/hub/en/security-tokens).\n",
        "- This token should not be disclosed to anyone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvPCgRbdXBAy",
        "outputId": "47700a68-6950-4cf3-db0d-17adfad15b20"
      },
      "outputs": [],
      "source": [
        "# hugging face token\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'YOUR_HUGGING_FACE_TOKEN' with your actual token\n",
        "login(token=\"YOUR_HUGGING_FACE_TOKEN\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFoYXCQra-cs"
      },
      "source": [
        "## Hugging Face Repository\n",
        "\n",
        "- Now I will create a repository on Hugging Face to store my model.\n",
        "- Replace the repo_name value with your repository name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "-Ez7BriSYO6z",
        "outputId": "66a10629-ad5f-4a28-d6e1-5bf0cf1108ae"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# Replace 'your-username' and 'new-model-repo' with your actual username and desired repository name\n",
        "repo_owner = \"YOUR_HF_ID\"\n",
        "repo_name = \"YOUR_REPO_NAME\"\n",
        "repo_id = f\"{repo_owner}/{repo_name}\"\n",
        "\n",
        "# Check if the repository exists\n",
        "try:\n",
        "    # Get the repository information\n",
        "    repo_info = api.repo_info(repo_id)\n",
        "    if repo_info:\n",
        "        # If the repository exists, delete it\n",
        "        api.delete_repo(repo_id)\n",
        "        print(f\"Repository '{repo_id}' exists and has been deleted.\")\n",
        "except Exception as e:\n",
        "    # If the repository does not exist, an exception will be raised\n",
        "    print(f\"Repository '{repo_id}' does not exist. Proceeding to create a new one.\")\n",
        "\n",
        "# Create a new repository\n",
        "api.create_repo(repo_id=repo_id)  # Set private=True if you want a private repository\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NKfOFwabcaQ"
      },
      "source": [
        "## Upload Model to Repo\n",
        "\n",
        "- I am now logged into my Hugging Face account. And a new repository is also created.\n",
        "- I will now push the fine-tuned model, tokenizer, and configuration files to my repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "d2f9a23b38c4486fbb6986157b38c978",
            "69322509686045f098d2748b3ece163e",
            "1e9a8795854e4d9ea2dff92c72aed44f",
            "62b65e45b6104f95803f05167cddcb2c",
            "7a190b4620d54dbc857239bd969c2280",
            "310fe31421694677af3d6d5a25a9088a",
            "1f98388e12384d719f165e363c74cf7c",
            "5dccd6d6a67a4746972fe471b27b7ae9",
            "1de7c1d70d7d48a99ea2418649d6bd2d",
            "9181f25409314cb8aa3fcfe508406001",
            "ab1fcec68467427f9793686665c5b515"
          ]
        },
        "id": "dcz0bbnwXOyQ",
        "outputId": "bb5febd2-7d96-4a4f-f4f5-6528d681411f"
      },
      "outputs": [],
      "source": [
        "# upload model to repo\n",
        "\n",
        "from huggingface_hub import upload_folder\n",
        "\n",
        "# Upload the model directory\n",
        "upload_folder(\n",
        "    folder_path=\"emotion-detection-model\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "# Upload the tokenizer directory\n",
        "upload_folder(\n",
        "    folder_path=\"emotion-detection-tokenizer\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjP6vCZMeg51"
      },
      "source": [
        "# Step 6 - Gradio Interface Development\n",
        "\n",
        "- The \"Emotion Detection App\" involves several steps and each step corresponds to a separate section in this Jupyter Notebook:\n",
        "\n",
        "1. Dataset Selection and Exploration\n",
        "2. Model Selection\n",
        "3. Model Fine-tuning\n",
        "4. Model Evaluation\n",
        "5. Upload Model to Hugging Face - `covered in previous section`\n",
        "6. Gradio Interface Development - `covered in this section`\n",
        "7. Deployment on Hugging Face Spaces\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqtQfmqYBQ9z"
      },
      "source": [
        "## Recap steps 1 to 5\n",
        "\n",
        "- Let me provide some detailed comments on what has each of the 5 steps achieved earlier:\n",
        "\n",
        "1. **Dataset Selection and Exploration**\n",
        "\n",
        "- I am working with the Emotion dataset from Hugging Face, which contains text inputs labeled with different emotions.\n",
        "- I explored the dataset to understand its structure and the distribution of different emotion classes.\n",
        "\n",
        "2. **Model Selection**\n",
        "\n",
        "- Then I selected a pre-trained model suitable for sequence classification tasks from Hugging Face's model hub.\n",
        "\n",
        "3. **Model Fine-tuning**\n",
        "\n",
        "- Then I fine-tuned the pre-trained model on our Emotion dataset to adapt it specifically for emotion detection in text.\n",
        "\n",
        "4. **Model Evaluation**\n",
        "\n",
        "   - I then evaluated the model for accuracy and precision.\n",
        "   - The results were in-line with my expectations.\n",
        "   - So I proceeded with uploading the model to Hugging Face\n",
        "\n",
        "5. **Upload Model to Hugging Face**\n",
        "\n",
        "   - In this section, I uploaded the fine-tuned and evaluated model to Hugging Face.\n",
        "   - I used the Hugging Face CLI so that I dont have to leave my Jupyter Notebook to do any manual steps.\n",
        "\n",
        "6. **Gradio Interface Development**\n",
        "\n",
        "   - This is the step now covered in this section.\n",
        "   - It involves developing a Gradio interface that will act as my UI for the model fine-tuned in previous steps.\n",
        "   - It is a simple interface that has just 2 boxes - one to put in your text and another to display the emotion as inferred by the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "cQp_rgoxez7l",
        "outputId": "0ddc687d-3ed2-4fa9-e893-9f9b480647e9"
      },
      "outputs": [],
      "source": [
        "# Step 6 - Gradio Interface Development\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"emotion-detection-model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emotion-detection-tokenizer\")\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = [\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
        "\n",
        "# Define a function to classify emotions in text\n",
        "def classify_emotion(text):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "    # Get model predictions\n",
        "    outputs = model(**inputs)\n",
        "    # Get the predicted class with the highest probability\n",
        "    predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    predicted_class = torch.argmax(predictions, dim=1).item()\n",
        "    # Get the predicted emotion label\n",
        "    predicted_emotion = emotion_labels[predicted_class]\n",
        "    return predicted_emotion\n",
        "\n",
        "# Link to my blog post\n",
        "blog_link = \"For more details about this project, visit my [blog post](https://www.kunal-pathak.com/blog/EmotionDetectionApp/).\"\n",
        "\n",
        "\n",
        "# Create a Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=classify_emotion,  # The function to call for predictions\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter text here...\"),  # Input component\n",
        "    outputs=gr.Textbox(),  # Output component\n",
        "    title=\"Emotion Detection in Text\",\n",
        "    description=\"Enter a sentence and the model will predict the emotion.\",\n",
        "    article=blog_link\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface with public sharing enabled\n",
        "interface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwLyUGgcC56J"
      },
      "source": [
        "# Step 7 - Deployment on Hugging Face Spaces\n",
        "\n",
        "- The \"Emotion Detection App\" involves several steps and each step corresponds to a separate section in this Jupyter Notebook:\n",
        "\n",
        "1. Dataset Selection and Exploration\n",
        "2. Model Selection\n",
        "3. Model Fine-tuning\n",
        "4. Model Evaluation\n",
        "5. Upload Model to Hugging Face\n",
        "6. Gradio Interface Development - `covered in previous section`\n",
        "7. Deployment on Hugging Face Spaces - `covered in this section`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtpIBom9DGJG"
      },
      "source": [
        "## All steps in Jupyter Notebook\n",
        "\n",
        "- Most of the time I go directly to the Hugging Face UI and then create my space and upload the model and do all other activities.\n",
        "- But I want to do the entire operation zero-touch. So the code in the next cell\n",
        "  - Creates the [Hugging Face Space](https://huggingface.co/docs/hub/en/spaces-overview) using the [CLI](https://huggingface.co/docs/huggingface_hub/en/guides/cli).\n",
        "  - Then clones the Space repository to my local/colab environment.\n",
        "  - Copies my Gradio app script, model files, and others dependencies to the cloned repository.\n",
        "  - Commits and pushes the changes to the Space repository, triggering the deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy5pXrX8Tg5m",
        "outputId": "b525c9fc-0110-49e3-d8f9-ef74dbb806a8"
      },
      "outputs": [],
      "source": [
        "# Step 7 - Deployment on Hugging Face Spaces\n",
        "\n",
        "# Import Libraries and Authenticate\n",
        "from huggingface_hub import HfApi, Repository, login\n",
        "import os  # Import the os module here\n",
        "\n",
        "# Authenticate with your Hugging Face token\n",
        "login(token=\"YOUR_HUGGING_FACE_TOKEN\", add_to_git_credential=True)\n",
        "\n",
        "# Create a New Space (skip this if the space already exists)\n",
        "api = HfApi()\n",
        "space_name = \"emotion-detection\"\n",
        "username = \"YOUR_HF_USER_NAME\"  # Replace with your Hugging Face username\n",
        "\n",
        "try:\n",
        "    # Try to create the Space\n",
        "    api.create_repo(repo_id=f\"{username}/{space_name}\", repo_type=\"space\", space_sdk=\"gradio\")\n",
        "except Exception as e:\n",
        "    print(f\"Repository already exists: {e}\")\n",
        "\n",
        "# Remove the local directory if it exists\n",
        "import shutil\n",
        "if os.path.exists(space_name):\n",
        "    shutil.rmtree(space_name)\n",
        "\n",
        "# Clone the Repository with Authentication\n",
        "!git clone https://huggingface.co/spaces/{username}/{space_name}\n",
        "\n",
        "# Change directory to the cloned repo\n",
        "os.chdir(space_name)\n",
        "\n",
        "# Create the Gradio app script and requirements.txt\n",
        "app_code = \"\"\"\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"kanad13/emotion_detection\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"kanad13/emotion_detection\")\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = [\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
        "\n",
        "# Define a function to classify emotions in text\n",
        "def classify_emotion(text):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "    # Get model predictions\n",
        "    outputs = model(**inputs)\n",
        "    # Get the predicted class with the highest probability\n",
        "    predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    predicted_class = torch.argmax(predictions, dim=1).item()\n",
        "    # Get the predicted emotion label\n",
        "    predicted_emotion = emotion_labels[predicted_class]\n",
        "    return predicted_emotion\n",
        "\n",
        "# Link to my blog post\n",
        "blog_link = \"For more details about this project, visit my [blog post](https://www.kunal-pathak.com/blog/EmotionDetectionApp/).\"\n",
        "\n",
        "# Create a Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=classify_emotion,  # The function to call for predictions\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter text here...\"),  # Input component\n",
        "    outputs=gr.Textbox(),  # Output component\n",
        "    title=\"Emotion Detection in Text\",\n",
        "    description=\"Enter a sentence and the model will predict the emotion.\",\n",
        "    article=blog_link\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()\n",
        "\"\"\"\n",
        "\n",
        "requirements_txt = \"\"\"\n",
        "datasets\n",
        "pandas\n",
        "matplotlib\n",
        "transformers\n",
        "IProgress\n",
        "accelerate\n",
        "huggingface_hub\n",
        "gradio\n",
        "torch\n",
        "\"\"\"\n",
        "\n",
        "# Write the Gradio app script to app.py\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# Write the requirements.txt\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements_txt)\n",
        "\n",
        "# Pull the remote changes to avoid conflicts\n",
        "!git pull origin main\n",
        "\n",
        "# Push the Changes to Hugging Face Spaces\n",
        "!git config --global user.email \"YOUR_EMAIL_ID\"\n",
        "!git config --global user.name \"YOUR_USER_NAME\"\n",
        "\n",
        "!git add app.py requirements.txt\n",
        "!git commit -m \"Add Gradio app script and requirements\"\n",
        "\n",
        "# Pull remote changes to avoid conflicts before pushing\n",
        "!git pull --rebase origin main\n",
        "\n",
        "# Push using the token for authentication\n",
        "!git remote set-url origin https://huggingface.co/spaces/{username}/{space_name}\n",
        "!git push\n",
        "\n",
        "# Navigate back to the original directory\n",
        "os.chdir(\"..\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1de7c1d70d7d48a99ea2418649d6bd2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e9a8795854e4d9ea2dff92c72aed44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dccd6d6a67a4746972fe471b27b7ae9",
            "max": 267844872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1de7c1d70d7d48a99ea2418649d6bd2d",
            "value": 267844872
          }
        },
        "1f98388e12384d719f165e363c74cf7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e4d6cff7c6649178354476c86fa2edf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "310fe31421694677af3d6d5a25a9088a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dccd6d6a67a4746972fe471b27b7ae9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62b65e45b6104f95803f05167cddcb2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9181f25409314cb8aa3fcfe508406001",
            "placeholder": "",
            "style": "IPY_MODEL_ab1fcec68467427f9793686665c5b515",
            "value": "268M/268M[00:07&lt;00:00,47.2MB/s]"
          }
        },
        "6600f3b00ecb414492a6685a281aa8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69322509686045f098d2748b3ece163e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_310fe31421694677af3d6d5a25a9088a",
            "placeholder": "",
            "style": "IPY_MODEL_1f98388e12384d719f165e363c74cf7c",
            "value": "model.safetensors:100%"
          }
        },
        "76e99c8c940e473f9e2481cd7a6af196": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_daffd17cc11548e680aeb15fe2342fe6",
              "IPY_MODEL_ec9e683e0ac84ad98b22b977a7b5c99f",
              "IPY_MODEL_a00fdfa5e4ca496aa434cab686eb6db1"
            ],
            "layout": "IPY_MODEL_c9dcce5da95e4ea0ac3ff4a04ef91ca4"
          }
        },
        "7a190b4620d54dbc857239bd969c2280": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7acb2ba9265746d0beab32e99ef272f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86f27d0a15c74bc0b2b408ba7f1685cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9181f25409314cb8aa3fcfe508406001": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a00fdfa5e4ca496aa434cab686eb6db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86f27d0a15c74bc0b2b408ba7f1685cb",
            "placeholder": "",
            "style": "IPY_MODEL_7acb2ba9265746d0beab32e99ef272f3",
            "value": "10/10[00:00&lt;00:00,105.37examples/s]"
          }
        },
        "ab1fcec68467427f9793686665c5b515": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9dcce5da95e4ea0ac3ff4a04ef91ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2f9a23b38c4486fbb6986157b38c978": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69322509686045f098d2748b3ece163e",
              "IPY_MODEL_1e9a8795854e4d9ea2dff92c72aed44f",
              "IPY_MODEL_62b65e45b6104f95803f05167cddcb2c"
            ],
            "layout": "IPY_MODEL_7a190b4620d54dbc857239bd969c2280"
          }
        },
        "daffd17cc11548e680aeb15fe2342fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3e5334ac6334769bec43e77884c2835",
            "placeholder": "",
            "style": "IPY_MODEL_de7e220a468a40fdadb6c2c66748ae07",
            "value": "Map:100%"
          }
        },
        "de7e220a468a40fdadb6c2c66748ae07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec9e683e0ac84ad98b22b977a7b5c99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e4d6cff7c6649178354476c86fa2edf",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6600f3b00ecb414492a6685a281aa8b8",
            "value": 10
          }
        },
        "f3e5334ac6334769bec43e77884c2835": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
